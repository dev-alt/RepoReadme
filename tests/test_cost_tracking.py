#!/usr/bin/env python3
"""
Test script for OpenRouter cost tracking accuracy.
This script verifies that our cost tracking system properly calculates
actual costs from API responses.
"""

import sys
import os

# Add src directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from openrouter_service import OpenRouterAIService, OpenRouterConfig, EnhancementRequest

def test_cost_calculation():
    """Test the cost calculation functionality."""
    print("üß™ Testing OpenRouter Cost Tracking System")
    print("=" * 50)
    
    # Test cost calculation method
    service = OpenRouterAIService()
    
    # Test with sample API response data
    sample_usage = {
        'prompt_tokens': 150,
        'completion_tokens': 75,
        'total_tokens': 225
    }
    
    # Test for different models
    test_models = [
        'openai/gpt-4',
        'anthropic/claude-3-sonnet'
    ]
    
    for model in test_models:
        print(f"\nüìä Testing cost calculation for {model}")
        try:
            cost_result = service.calculate_actual_cost(
                usage_data=sample_usage,
                model_id=model
            )
            
            # Also get the estimate for comparison
            cost_estimate = service.estimate_enhancement_cost(
                "Sample bio text for testing cost estimation accuracy", 
                model
            )
            
            if 'error' not in cost_result:
                print(f"   ‚Ä¢ Prompt tokens: {sample_usage['prompt_tokens']}")
                print(f"   ‚Ä¢ Completion tokens: {sample_usage['completion_tokens']}")
                print(f"   ‚Ä¢ Actual cost: ${cost_result['total_cost']:.8f}")
                
                if 'error' not in cost_estimate:
                    estimated = float(cost_estimate['cost_formatted'].replace('$', ''))
                    print(f"   ‚Ä¢ Estimated cost: ${estimated:.8f}")
                    print(f"   ‚Ä¢ Difference: ${abs(cost_result['total_cost'] - estimated):.8f}")
                    
                    if max(cost_result['total_cost'], estimated) > 0:
                        accuracy = (1 - abs(cost_result['total_cost'] - estimated) / max(cost_result['total_cost'], estimated)) * 100
                        print(f"   ‚Ä¢ Accuracy: {accuracy:.1f}%")
                else:
                    print(f"   ‚Ä¢ Estimate error: {cost_estimate.get('error', 'Unknown')}")
            else:
                print(f"   ‚Ä¢ Error: {cost_result.get('error', 'Unknown')}")
            
        except Exception as e:
            print(f"   ‚Ä¢ Exception: {e}")
    
    print(f"\n‚úÖ Cost tracking test completed!")
    print("\nNote: To test with real API calls, configure your OpenRouter API key")
    print("in the main application and try the bio enhancement feature.")

def test_enhancement_result_structure():
    """Test the EnhancementResult data structure."""
    print(f"\nüîç Testing EnhancementResult Structure")
    print("=" * 50)
    
    from openrouter_service import EnhancementResult
    
    # Create a sample result with new cost tracking fields
    result = EnhancementResult(
        enhanced_bio="Sample enhanced bio",
        enhancement_score=85.5,
        improvements_made=["Improved clarity", "Added metrics"],
        suggestions=["Consider adding specific achievements"],
        model_used="openai/gpt-4o",
        tokens_used=225,
        processing_time=2.5,
        readability_improvement=15.0,
        engagement_improvement=25.0,
        keyword_optimization=10.0,
        actual_cost=0.001125,
        prompt_tokens=150,
        completion_tokens=75,
        generation_id="gen-abc123xyz"
    )
    
    print("‚úÖ EnhancementResult created successfully with fields:")
    print(f"   ‚Ä¢ enhanced_bio: {len(result.enhanced_bio)} chars")
    print(f"   ‚Ä¢ enhancement_score: {result.enhancement_score}")
    print(f"   ‚Ä¢ actual_cost: ${result.actual_cost:.6f}")
    print(f"   ‚Ä¢ prompt_tokens: {result.prompt_tokens}")
    print(f"   ‚Ä¢ completion_tokens: {result.completion_tokens}")
    print(f"   ‚Ä¢ generation_id: {result.generation_id}")
    print(f"   ‚Ä¢ total tokens: {result.tokens_used}")

if __name__ == "__main__":
    test_cost_calculation()
    test_enhancement_result_structure()
    
    print(f"\nüéâ All tests completed!")
    print("Ready for production use with accurate cost tracking.")